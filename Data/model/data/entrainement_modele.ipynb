{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Projet 5 : Entraînement du Modèle de Prédiction de Démission\n",
    "\n",
    "Ce notebook reprend la logique du Projet 4 pour nettoyer les données, créer des features, entraîner des modèles et sauvegarder le meilleur modèle pour le déploiement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f20f1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac7613",
   "metadata": {},
   "source": [
    "## 2. Prétraitement et Fusion\n",
    "Nous nettoyons les identifiants et fusionnons les tables pour obtenir un dataset unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298b7aa",
   "metadata": {},
   "source": [
    "## 3. Ingénierie des Fonctionnalités (Feature Engineering)\n",
    "Création des variables explicatives (ratios) et encodage des données catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fcf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ingénierie des Fonctionnalités ---\n",
      "Feature Engineering terminé.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Ingénierie des Fonctionnalités ---\")\n",
    "\n",
    "df = pd.read_csv(\"master_dataset.csv\")\n",
    "\n",
    "# Nettoyage des chaînes de caractères (ex: '11 %' -> 11.0)\n",
    "if df['augmentation_salaire_precedente'].dtype == 'object':\n",
    "    df['augmentation_salaire_precedente'] = df['augmentation_salaire_precedente'].astype(str).str.replace(' %', '').astype(float)\n",
    "\n",
    "# Encodage Binaire (Oui/Non -> 1/0)\n",
    "binary_map = {'Oui': 1, 'Non': 0}\n",
    "cols_binaires = ['heure_supplementaires']\n",
    "for col in cols_binaires:\n",
    "    if col in df.columns and df[col].dtype == 'object':\n",
    "        df[col] = df[col].map(binary_map)\n",
    "\n",
    "# Encodage Ordinal\n",
    "map_deplacement = {'Aucun': 0, 'Occasionnel': 1, 'Frequent': 2}\n",
    "if 'frequence_deplacement' in df.columns and df['frequence_deplacement'].dtype == 'object':\n",
    "    df['frequence_deplacement'] = df['frequence_deplacement'].map(map_deplacement)\n",
    "\n",
    "# Création de Ratios Métier\n",
    "# Remplacement des 0 par 1 pour éviter les divisions par zéro\n",
    "df['ratio_stagnation'] = df['annees_dans_le_poste_actuel'] / df['annees_dans_l_entreprise'].replace(0, 1)\n",
    "df['revenu_par_annee_exp'] = df['revenu_mensuel'] / df['annee_experience_totale'].replace(0, 1)\n",
    "\n",
    "# Définition de la Cible (Target)\n",
    "y = df['a_quitte_l_entreprise'].map({'Oui': 1, 'Non': 0})\n",
    "\n",
    "# Encodage One-Hot pour les autres variables catégorielles\n",
    "X_raw = df.drop(columns=['a_quitte_l_entreprise'])\n",
    "X_encoded = pd.get_dummies(X_raw, drop_first=True)\n",
    "\n",
    "# Suppression de l'identifiant (non prédictif)\n",
    "X = X_encoded.drop(columns=['id_employee'])\n",
    "\n",
    "print(\"Feature Engineering terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252504b",
   "metadata": {},
   "source": [
    "## 4. Sélection des Features Essentielles\n",
    "Nous ne conservons que les 10 variables les plus pertinentes identifiées lors de l'analyse exploratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cca03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement sur 10 features essentielles.\n"
     ]
    }
   ],
   "source": [
    "features_to_analyze = [\n",
    "    'revenu_mensuel',\n",
    "    'age',\n",
    "    'distance_domicile_travail',\n",
    "    'satisfaction_employee_environnement',\n",
    "    'heure_supplementaires',\n",
    "    'annees_depuis_la_derniere_promotion',\n",
    "    'satisfaction_employee_equilibre_pro_perso',\n",
    "    'nombre_participation_pee',\n",
    "    'ratio_stagnation',\n",
    "    'revenu_par_annee_exp'\n",
    "]\n",
    "\n",
    "# Vérification de la présence des colonnes\n",
    "available_features = [f for f in features_to_analyze if f in X.columns]\n",
    "if len(available_features) < len(features_to_analyze):\n",
    "    print(\"Attention : Certaines features essentielles sont manquantes.\")\n",
    "\n",
    "X_essential = X[available_features]\n",
    "print(f\"Entraînement sur {X_essential.shape[1]} features essentielles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba6592",
   "metadata": {},
   "source": [
    "## 5. Entraînement des Modèles (Baseline)\n",
    "Nous comparons un Dummy Classifier, une Régression Logistique et un Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73094155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Séparation Train/Test ---\n",
      "\n",
      "========================================\n",
      "ÉVALUATION : Dummy Classifier\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       247\n",
      "           1       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.42      0.50      0.46       294\n",
      "weighted avg       0.71      0.84      0.77       294\n",
      "\n",
      "Score ROC-AUC : 0.5000\n",
      "\n",
      "========================================\n",
      "ÉVALUATION : Régression Logistique (Base)\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91       247\n",
      "           1       0.53      0.17      0.26        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.70      0.57      0.59       294\n",
      "weighted avg       0.81      0.84      0.81       294\n",
      "\n",
      "Score ROC-AUC : 0.7739\n",
      "\n",
      "========================================\n",
      "ÉVALUATION : Random Forest (Base)\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       247\n",
      "           1       0.48      0.21      0.29        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.67      0.58      0.60       294\n",
      "weighted avg       0.80      0.84      0.81       294\n",
      "\n",
      "Score ROC-AUC : 0.7626\n"
     ]
    }
   ],
   "source": [
    "def evaluer_modele(model, X_test, y_test, model_name):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"ÉVALUATION : {model_name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    if y_prob is not None:\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        print(f\"Score ROC-AUC : {auc:.4f}\")\n",
    "\n",
    "print(\"--- Séparation Train/Test ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_essential, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 1. Dummy Classifier\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train, y_train)\n",
    "evaluer_modele(dummy, X_test, y_test, \"Dummy Classifier\")\n",
    "\n",
    "# 2. Régression Logistique\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "evaluer_modele(log_reg, X_test, y_test, \"Régression Logistique (Base)\")\n",
    "\n",
    "# 3. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "evaluer_modele(rf, X_test, y_test, \"Random Forest (Base)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ae1fb",
   "metadata": {},
   "source": [
    "## 6. Optimisation (GridSearch + SMOTE)\n",
    "Nous optimisons le Random Forest pour maximiser le rappel (Recall), car il est crucial de ne pas rater un démissionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8360b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimisation des Hyperparamètres ---\n",
      "Recherche des meilleurs paramètres...\n",
      "Meilleurs paramètres : {'model__class_weight': 'balanced', 'model__max_depth': 5, 'model__min_samples_leaf': 4, 'model__n_estimators': 50}\n",
      "\n",
      "========================================\n",
      "ÉVALUATION : Random Forest (Optimisé)\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.85       247\n",
      "           1       0.33      0.47      0.39        47\n",
      "\n",
      "    accuracy                           0.76       294\n",
      "   macro avg       0.61      0.64      0.62       294\n",
      "weighted avg       0.80      0.76      0.78       294\n",
      "\n",
      "Score ROC-AUC : 0.6839\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Optimisation des Hyperparamètres ---\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Pipeline avec SMOTE pour gérer le déséquilibre\n",
    "pipe_rf = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Grille de recherche\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [50, 100],\n",
    "    'model__max_depth': [5, 10, 15],\n",
    "    'model__min_samples_leaf': [2, 4],\n",
    "    'model__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "print(\"Recherche des meilleurs paramètres...\")\n",
    "grid_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=cv, scoring='recall', n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(f\"Meilleurs paramètres : {grid_rf.best_params_}\")\n",
    "evaluer_modele(best_rf, X_test, y_test, \"Random Forest (Optimisé)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e49a3",
   "metadata": {},
   "source": [
    "## 7. Optimisation Régression Logistique\n",
    "Nous optimisons également la Régression Logistique, qui offre souvent une meilleure interprétabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b936e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimisation Régression Logistique ---\n",
      "Recherche des meilleurs paramètres (LogReg)...\n",
      "Meilleurs paramètres LogReg : {'model__C': 0.1, 'model__class_weight': 'balanced', 'smote__k_neighbors': 1}\n",
      "\n",
      "========================================\n",
      "ÉVALUATION : Régression Logistique (Optimisée)\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81       247\n",
      "           1       0.32      0.66      0.43        47\n",
      "\n",
      "    accuracy                           0.72       294\n",
      "   macro avg       0.62      0.69      0.62       294\n",
      "weighted avg       0.82      0.72      0.75       294\n",
      "\n",
      "Score ROC-AUC : 0.7697\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Optimisation Régression Logistique ---\")\n",
    "\n",
    "# Pipeline : Scaling -> SMOTE -> Modèle\n",
    "pipe_log = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grille de paramètres\n",
    "param_grid_log = {\n",
    "    'model__C': [0.01, 0.1, 1, 5, 10],\n",
    "    'model__class_weight': ['balanced', None],\n",
    "    'smote__k_neighbors': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "print(\"Recherche des meilleurs paramètres (LogReg)...\")\n",
    "grid_log = GridSearchCV(pipe_log, param_grid_log, cv=cv, scoring='recall', n_jobs=-1)\n",
    "grid_log.fit(X_train, y_train)\n",
    "\n",
    "best_log_reg = grid_log.best_estimator_\n",
    "print(f\"Meilleurs paramètres LogReg : {grid_log.best_params_}\")\n",
    "evaluer_modele(best_log_reg, X_test, y_test, \"Régression Logistique (Optimisée)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ce736",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde du Modèle\n",
    "Nous sauvegardons le modèle **Régression Logistique** (qui a montré de meilleures performances) et la liste des features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc2781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sauvegarde des Artefacts ---\n",
      "Modèle et features sauvegardés dans '../Modele'.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Sauvegarde des Artefacts ---\")\n",
    "output_dir = '..'\n",
    "\n",
    "# Sauvegarde du modèle (Régression Logistique)\n",
    "# Note: best_log_reg est un Pipeline (Scaler -> SMOTE -> LogReg)\n",
    "joblib.dump(best_log_reg, f'{output_dir}/model.joblib')\n",
    "\n",
    "# Sauvegarde des features\n",
    "joblib.dump(X_essential.columns.tolist(), f'{output_dir}/features.joblib')\n",
    "\n",
    "print(f\"Modèle et features sauvegardés dans '{output_dir}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
